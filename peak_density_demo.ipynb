{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48df67-ddf2-4a9a-a26d-2d12c121f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9701fcd-5eaa-42ec-a1c8-feaef4942014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./model')\n",
    "from models import BayesianNeuralNetwork, train_bnn, predict_bnn\n",
    "from models import train_gpr, predict_gpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21591a-1b35-459a-beb2-b96627261c10",
   "metadata": {},
   "source": [
    "## Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac54a8-a5dc-43cd-9624-c18305212b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputdata(all_data):\n",
    "  NUM_OF_BINS = 502\n",
    "  input_data = []\n",
    "  output = []\n",
    "  errors = []\n",
    "  z_data = []\n",
    "\n",
    "  #exlclude_paras = {\"c\": [\"0.25\", \"0.75\", \"1.25\", \"1.75\"]}\n",
    "  exlclude_paras = {}\n",
    "  for key, data in all_data.items():\n",
    "    #print(key, data)\n",
    "    density_profiles = []\n",
    "    density_errors = []\n",
    "    z_data_values = []\n",
    "    input_names = key.split(\"_\")[0::2]\n",
    "    input_paras = key.split(\"_\")[1::2]\n",
    "\n",
    "    ignore_this = False\n",
    "    for key_p, params in exlclude_paras.items():\n",
    "        if input_paras[input_names.index(key_p)] in params:\n",
    "            ignore_this= True\n",
    "            break\n",
    "    if ignore_this:\n",
    "        continue\n",
    "\n",
    "    input_data.append(input_paras)\n",
    "    density_profiles.append(data['pos'][:,1])\n",
    "    density_profiles.append(data['neg'][:,1])\n",
    "    output.append(density_profiles)\n",
    "    density_errors.append(data['pos'][:,2])\n",
    "    density_errors.append(data['neg'][:,2])\n",
    "    errors.append(density_errors)\n",
    "    z_data_values.append(data['pos'][:,0])\n",
    "    z_data_values.append(data['neg'][:,0])\n",
    "    z_data.append(z_data_values)\n",
    "\n",
    "    #break\n",
    "\n",
    "  input_data = np.array(input_data)\n",
    "  output = np.array(output).reshape(-1,NUM_OF_BINS*2)\n",
    "  errors = np.array(errors).reshape(-1,NUM_OF_BINS*2)\n",
    "  z_data = np.array(z_data).reshape(-1,NUM_OF_BINS*2)\n",
    "  print(\"Input data shape: {}\".format(input_data.shape))\n",
    "  print(\"Output data shape: {}\".format(output.shape))\n",
    "  print(\"error bar data shape: {}\".format(errors.shape))\n",
    "  print(\"Bin center data shape: {}\".format(z_data.shape))\n",
    "\n",
    "  return input_data, output, errors, z_data\n",
    "\n",
    "\n",
    "def compute_peak_density(input_data, output, errors, z_data):\n",
    "    output_peak_density = np.zeros((input_data.shape[0], 1))\n",
    "    error_peak_density = np.zeros((input_data.shape[0], 1))\n",
    "    for i in range(input_data.shape[0]):\n",
    "        max_index = np.argmax(output[i])\n",
    "        output_peak_density[i] = output[i][max_index]\n",
    "        error_peak_density[i] = errors[i][max_index]\n",
    "    return output_peak_density, error_peak_density\n",
    "\n",
    "def calculate_rmse(actual_values, predicted_values):\n",
    "    mse = mean_squared_error(actual_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401901e-da79-4d2a-9ab3-6d96d417fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#active learning loop for GPR\n",
    "\n",
    "def active_learning_loop_GPR(xtrain, ytrain, xtest, ytest, xremain, yremain,\n",
    "                         kernel_variance, kernel_lengthscale, \n",
    "                         white_kernel_variance, max_iterations, epochs, n_samples=3):\n",
    "    # Initialize current datasets\n",
    "    current_xtrain = xtrain.copy()\n",
    "    current_ytrain = ytrain.copy()\n",
    "    current_xtest = xtest.copy()\n",
    "    current_ytest = ytest.copy()\n",
    "    current_xremain = xremain.copy()\n",
    "    current_yremain = yremain.copy()\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'rmse': [],\n",
    "        'training_size': [],\n",
    "        'test_size': [],\n",
    "        'std': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        print(f\"\\n--- Active Learning Epoch {epoch}/{epochs} ---\")\n",
    "        print(f\"Training samples: {len(current_xtrain)}, Test samples: {len(current_xtest)}\")\n",
    "        \n",
    "        # Train GPR model\n",
    "        start_time = time.time()\n",
    "        model = train_gpr(current_xtrain, current_ytrain, \n",
    "                          kernel_variance, kernel_lengthscale,\n",
    "                          white_kernel_variance, max_iterations)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict on current test set\n",
    "        mean, std = predict_gpr(model, current_xtest)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = calculate_rmse(current_ytest, mean)\n",
    "        metrics['rmse'].append(rmse)\n",
    "        metrics['training_size'].append(len(current_xtrain))\n",
    "        metrics['test_size'].append(len(current_xtest))\n",
    "        metrics['std'].append(np.mean(std))\n",
    "        \n",
    "        print(f\"Training time: {train_time:.2f} seconds | RMSE: {rmse:.4f}\")\n",
    "        print(f\"Prediction stats: Mean={np.mean(mean):.4f} ± Std={np.mean(std):.4f}\")\n",
    "\n",
    "        #Evaluate on the remaining dataset pool\n",
    "        mean_remain, std_remain = predict_gpr(model, current_xremain)\n",
    "       \n",
    "        # Break if remain set is exhausted\n",
    "        if len(current_xremain) <= n_samples:\n",
    "            print(\"Remain set exhausted. Stopping early.\")\n",
    "            break\n",
    "        \n",
    "        # Identify top N uncertain samples (highest std)\n",
    "        std_flattened = std_remain.flatten()\n",
    "        topN_indices = np.argsort(std_flattened)[-n_samples:]\n",
    "        \n",
    "        # Add to training set\n",
    "        current_xtrain = np.vstack([current_xtrain, current_xremain[topN_indices]])\n",
    "        current_ytrain = np.vstack([current_ytrain, current_yremain[topN_indices]])\n",
    "        \n",
    "        # Remove from remain set\n",
    "        mask = np.ones(len(current_xremain), dtype=bool)\n",
    "        mask[topN_indices] = False\n",
    "        current_xremain = current_xremain[mask]\n",
    "        current_yremain = current_yremain[mask]\n",
    "\n",
    "    \n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf11913-5fab-4125-b819-f7d3e70672a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#active learning loop for BNN\n",
    "\n",
    "def active_learning_loop_BNN(xtrain, ytrain, xtest, ytest, xremain, yremain,\n",
    "                         n_features, hidden_layers, weight_init_std, \n",
    "                         log_std_init_mean, log_std_init_std, log_std_clamp,\n",
    "                         bnn_epochs, learning_rate, grad_clip_norm,\n",
    "                         al_epochs, n_samples=5, n_mc_samples=100, batch_size=32):\n",
    "    # Convert to PyTorch tensors\n",
    "    xtrain_t = torch.FloatTensor(xtrain)\n",
    "    ytrain_t = torch.FloatTensor(ytrain)\n",
    "    xtest_t = torch.FloatTensor(xtest)\n",
    "    ytest_t = torch.FloatTensor(ytest)\n",
    "    xremain_t = torch.FloatTensor(xremain)\n",
    "    yremain_t = torch.FloatTensor(yremain)\n",
    "    \n",
    "    # Initialize current datasets\n",
    "    current_xtrain = xtrain_t.clone()\n",
    "    current_ytrain = ytrain_t.clone()\n",
    "    current_xtest = xtest_t.clone()\n",
    "    current_ytest = ytest_t.clone()\n",
    "    current_xremain = xremain_t.clone()\n",
    "    current_yremain = yremain_t.clone()\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'rmse': [],\n",
    "        'training_size': [],\n",
    "        'test_size': [],\n",
    "        'std': []\n",
    "    }\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for epoch in range(al_epochs+1):\n",
    "        print(f\"\\n--- Active Learning Epoch {epoch}/{al_epochs} ---\")\n",
    "        print(f\"Training samples: {len(current_xtrain)}, Test samples: {len(current_xtest)}\")\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(current_xtrain, current_ytrain)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize and train BNN\n",
    "        model = BayesianNeuralNetwork(\n",
    "            n_features, hidden_layers, weight_init_std,\n",
    "            log_std_init_mean, log_std_init_std, log_std_clamp\n",
    "        ).to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_bnn(model, train_loader, bnn_epochs, learning_rate, grad_clip_norm)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_preds, test_std = predict_bnn(model, current_xtest.to(device), n_samples=n_mc_samples)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = calculate_rmse(current_ytest.numpy(), test_preds.cpu().numpy())\n",
    "        metrics['rmse'].append(rmse)\n",
    "        metrics['training_size'].append(len(current_xtrain))\n",
    "        metrics['test_size'].append(len(current_xtest))\n",
    "        metrics['std'].append(test_std.mean().item())\n",
    "        \n",
    "        print(f\"Training time: {train_time:.2f} seconds | RMSE: {rmse:.4f}\")\n",
    "        print(f\"Prediction stats: Mean={test_preds.mean().item():.4f} ± Std={test_std.mean().item():.4f}\")\n",
    "\n",
    "        # Break if remain set is exhausted\n",
    "        if len(current_xremain) <= n_samples:\n",
    "            print(\"Remain set exhausted.\")\n",
    "            break\n",
    "            \n",
    "        # Predict uncertainty on remaining pool\n",
    "        with torch.no_grad():\n",
    "            _, remain_std = predict_bnn(model, current_xremain.to(device), n_samples=n_mc_samples)\n",
    "        \n",
    "        # Identify top N uncertain samples\n",
    "        std_np = remain_std.cpu().numpy()\n",
    "        topN_indices = np.argsort(std_np)[-n_samples:]\n",
    "        \n",
    "        # Add to training set\n",
    "        current_xtrain = torch.cat([current_xtrain, current_xremain[topN_indices]])\n",
    "        current_ytrain = torch.cat([current_ytrain, current_yremain[topN_indices]])\n",
    "        \n",
    "        # Remove from remain set\n",
    "        mask = torch.ones(len(current_xremain), dtype=torch.bool)\n",
    "        mask[topN_indices] = False\n",
    "        current_xremain = current_xremain[mask]\n",
    "        current_yremain = current_yremain[mask]\n",
    "\n",
    "    return metrics, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#active learning for MCD\n",
    "def active_learning_loop_MCD(xtrain, ytrain, xtest, ytest, xremain, yremain,\n",
    "                             n_features, hidden_units, dropout_rate,\n",
    "                             mcd_epochs, learning_rate, al_epochs,\n",
    "                             n_samples=5, T=100, batch_size=32):\n",
    "    # Convert to PyTorch tensors\n",
    "    xtrain_t = torch.FloatTensor(xtrain)\n",
    "    ytrain_t = torch.FloatTensor(ytrain)\n",
    "    xtest_t = torch.FloatTensor(xtest)\n",
    "    ytest_t = torch.FloatTensor(ytest)\n",
    "    xremain_t = torch.FloatTensor(xremain)\n",
    "    yremain_t = torch.FloatTensor(yremain)\n",
    "    \n",
    "    # Initialize current datasets\n",
    "    current_xtrain = xtrain_t.clone()\n",
    "    current_ytrain = ytrain_t.clone()\n",
    "    current_xtest = xtest_t.clone()\n",
    "    current_ytest = ytest_t.clone()\n",
    "    current_xremain = xremain_t.clone()\n",
    "    current_yremain = yremain_t.clone()\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics = {\n",
    "        'rmse': [],\n",
    "        'training_size': [],\n",
    "        'test_size': [],\n",
    "        'std': []\n",
    "    }\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create test DataLoader for prediction\n",
    "    test_dataset = TensorDataset(current_xtest, current_ytest)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create remain DataLoader for uncertainty sampling\n",
    "    remain_dataset = TensorDataset(current_xremain, current_yremain)\n",
    "    remain_loader = DataLoader(remain_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(al_epochs + 1):\n",
    "        print(f\"\\n--- Active Learning Epoch {epoch}/{al_epochs} ---\")\n",
    "        print(f\"Training samples: {len(current_xtrain)}, Remain pool: {len(current_xremain)}\")\n",
    "        \n",
    "        # Create DataLoader for current training set\n",
    "        train_dataset = TensorDataset(current_xtrain, current_ytrain)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize MCD model\n",
    "        model = DropoutModel(\n",
    "            n_features, \n",
    "            hidden_units[0], \n",
    "            hidden_units[1] if len(hidden_units) > 1 else hidden_units[0],\n",
    "            hidden_units[2] if len(hidden_units) > 2 else hidden_units[0],\n",
    "            dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train MCD model\n",
    "        start_time = time.time()\n",
    "        train_mcd(model, train_loader, mcd_epochs, learning_rate)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_mean, test_std = predict_mcd(model, test_loader, T)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = calculate_rmse(current_ytest.numpy(), test_mean)\n",
    "        metrics['rmse'].append(rmse)\n",
    "        metrics['training_size'].append(len(current_xtrain))\n",
    "        metrics['test_size'].append(len(current_xtest))\n",
    "        metrics['std'].append(test_std.mean())\n",
    "        \n",
    "        print(f\"Training time: {train_time:.2f} seconds | RMSE: {rmse:.6f}\")\n",
    "        print(f\"Prediction stats: Mean={test_mean.mean():.6f} ± Std={test_std.mean():.6f}\")\n",
    "\n",
    "        # Break if remain set is exhausted\n",
    "        if len(current_xremain) <= n_samples or epoch == al_epochs:\n",
    "            print(\"Remain pool exhausted or final epoch reached\")\n",
    "            break\n",
    "            \n",
    "        # Predict uncertainty on remaining pool\n",
    "        remain_mean, remain_std = predict_mcd(model, remain_loader, T)\n",
    "        \n",
    "        # Identify top N uncertain samples\n",
    "        topN_indices = np.argsort(remain_std)[-n_samples:]\n",
    "        \n",
    "        # Add to training set\n",
    "        current_xtrain = torch.cat([current_xtrain, current_xremain[topN_indices]])\n",
    "        current_ytrain = torch.cat([current_ytrain, current_yremain[topN_indices]])\n",
    "        \n",
    "        # Remove from remain set\n",
    "        mask = torch.ones(len(current_xremain), dtype=torch.bool)\n",
    "        mask[topN_indices] = False\n",
    "        current_xremain = current_xremain[mask]\n",
    "        current_yremain = current_yremain[mask]\n",
    "        \n",
    "        # Update remain DataLoader\n",
    "        remain_dataset = TensorDataset(current_xremain, current_yremain)\n",
    "        remain_loader = DataLoader(remain_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0431bb-4568-4213-8072-3aeb40118014",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bd54b-5997-4f4a-b230-d5a82871cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= 'data/'\n",
    "with open(file_path+ 'data_dump_density_preprocessed_train.pk', 'rb') as handle:\n",
    "    processed_all_data_preprocessed_train = pickle.load(handle)\n",
    "with open(file_path+ 'data_dump_density_preprocessed_test.pk', 'rb') as handle:\n",
    "    processed_all_data_preprocessed_test = pickle.load(handle)\n",
    "\n",
    "#reduce training set size by randomly excluding N data.\n",
    "np.random.seed(0)   #fix random seed\n",
    "index_ = np.random.choice(len(processed_all_data_preprocessed_train.keys()), 3500,replace=False)  #change choice size to select reduced training samples\n",
    "excluded_index_ = np.delete(np.arange(0,len(processed_all_data_preprocessed_train.keys())), index_)\n",
    "train_ = {}\n",
    "exclude_ = {}\n",
    "for index in index_:\n",
    "    exclude_[list(processed_all_data_preprocessed_train.keys())[index]] = processed_all_data_preprocessed_train[list(processed_all_data_preprocessed_train.keys())[index]]\n",
    "for index in excluded_index_:\n",
    "    train_[list(processed_all_data_preprocessed_train.keys())[index]] = processed_all_data_preprocessed_train[list(processed_all_data_preprocessed_train.keys())[index]]\n",
    "\n",
    "\n",
    "#Preprocess data to density output (NX1004)\n",
    "input_data, output, errors, z_data = preprocess_inputdata(train_)\n",
    "input_data_remain, output_remain, errors_remain, z_data_remain = preprocess_inputdata(exclude_)\n",
    "input_data_test, output_test_raw, errors_test_raw, z_data_test = preprocess_inputdata(processed_all_data_preprocessed_test)\n",
    "\n",
    "\n",
    "#Covert to peak density output (NX1)\n",
    "output_train, errors_train = compute_peak_density(input_data, output, errors, z_data)\n",
    "output_train_remain, errors_train_remain = compute_peak_density(input_data_remain, output_remain, errors_remain, z_data_remain)\n",
    "output_test, errors_test = compute_peak_density(input_data_test, output_test_raw, errors_test_raw, z_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d72c9d-d6c1-418a-9e4e-080fd9dc0f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation \n",
    "#split ranges 0.8 to 1\n",
    "train_test_split = 1\n",
    "seed = 65231\n",
    "\n",
    "\n",
    "input_data_suff, output_suff,  errors_suff, z_data_shuff = shuffle(input_data, output_train, errors_train, z_data, random_state=seed)\n",
    "#input_data_suff, output_suff,  errors_suff, z_data_shuff = shuffle(input_data, output[:, :100], errors[:, :100], z_data[:, :100], random_state=seed)\n",
    "\n",
    "train_test_split_ = int(input_data_suff.shape[0]*train_test_split)\n",
    "\n",
    "x_train = input_data_suff[0:train_test_split_]#.astype(\"float64\")\n",
    "x_test = input_data_suff[train_test_split_:]#.astype(\"float64\")\n",
    "\n",
    "\n",
    "y_train = output_suff[0:train_test_split_]#.astype(\"float64\")\n",
    "y_test = output_suff[train_test_split_:]#.astype(\"float64\")\n",
    "\n",
    "\n",
    "error_train = errors_suff[0:train_test_split_]#.astype(\"float64\")\n",
    "error_test = errors_suff[train_test_split_:]#.astype(\"float64\")\n",
    "\n",
    "z_data_train = z_data_shuff[0:train_test_split_]#.astype(\"float64\")\n",
    "z_data_test = z_data_shuff[train_test_split_:]#.astype(\"float64\")\n",
    "\n",
    "\n",
    "#x_train, x_test, y_train, y_test = spliter.train_test_split(input_data, output, test_size=(1-train_test_split), random_state=100)\n",
    "\n",
    "print(\"Train input: \", x_train.shape)\n",
    "print(\"Train Output\", y_train.shape)\n",
    "print(\"Test input: \", x_test.shape)\n",
    "print(\"Test Output\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6cc8c-b5ae-4e85-a9ca-75ffbdd3d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load(file_path+'scaler_new.pkl')\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "scaled_x_test = scaler.transform(input_data_test)\n",
    "\n",
    "\n",
    "scaler_y = joblib.load(file_path+'minmax_scaler_peak_label.joblib')\n",
    "scaler_error = joblib.load(file_path+'minmax_scaler_peak_error.joblib')\n",
    "\n",
    "scaled_y_train = scaler_y.transform(y_train)\n",
    "scaled_y_test = scaler_y.transform(output_test)\n",
    "scaled_error_train =  scaler_error.transform(error_train)\n",
    "scaled_error_test =  scaler_error.transform(errors_test)\n",
    "\n",
    "\n",
    "scaled_x_remain = scaler.transform(input_data_remain)\n",
    "scaled_y_remain = scaler_y.transform(output_train_remain)\n",
    "scaled_error_remain = scaler_error.transform(errors_train_remain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91be9f-7743-46e0-be79-efcbe4321e53",
   "metadata": {},
   "source": [
    "## GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf754de6-31fc-48e5-a703-957b9e0cbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_VARIANCE = 1\n",
    "KERNEL_LENGTHSCALE = 1\n",
    "WHITE_KERNEL_VARIANCE = 1\n",
    "MAX_ITERATIONS = 2000\n",
    "EPOCHS = 100\n",
    "N_SAMPLES = 5  # Number of samples to add per epoch\n",
    "\n",
    "# Run active learning\n",
    "metrics, final_model = active_learning_loop_GPR(\n",
    "    scaled_x_train, scaled_y_train,\n",
    "    scaled_x_test, scaled_y_test,\n",
    "    scaled_x_remain, scaled_y_remain,\n",
    "    KERNEL_VARIANCE, KERNEL_LENGTHSCALE,\n",
    "    WHITE_KERNEL_VARIANCE, MAX_ITERATIONS, \n",
    "    EPOCHS, N_SAMPLES\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Active Learning Summary ===\")\n",
    "print(f\"Initial Training Size: {len(scaled_x_train)}\")\n",
    "print(f\"Final Training Size: {metrics['training_size'][-1]}\")\n",
    "print(f\"Final Test Size: {metrics['test_size'][-1]}\")\n",
    "print(f\"Minimum RMSE: {min(metrics['rmse']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01101ea-2d57-4114-9665-1756d54c6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [50, 100, 150, 200, 250, 300, 400, 550]\n",
    "RMSE = [0.0287, 0.0194,0.0152, 0.0122, 0.0101, 0.0093, 0.0074, 0.0062 ]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics['training_size'], metrics['rmse'], \n",
    "         marker='o', markersize=6, \n",
    "         linestyle='-', color='red', \n",
    "         label='Active Learning', zorder=2)\n",
    "plt.plot(training_set, RMSE, \n",
    "         marker='s', markersize=6, \n",
    "         linestyle='-', color='blue', \n",
    "         label='Random Drop', zorder=4)\n",
    "\n",
    "plt.axhline(y=0.0062, color='k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Training Size', fontsize=16)\n",
    "plt.ylabel('RMSE on Testing Set', fontsize=16)\n",
    "plt.title('Active Learning vs Random Droping', fontsize=14)\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('al.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96b1a4-db91-47d3-935c-234268edf523",
   "metadata": {},
   "source": [
    "## BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd941f6-2912-4aee-854b-a5d9bd062bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYERS = [50, 50]\n",
    "WEIGHT_INIT_STD = 0.1\n",
    "LOG_STD_INIT_MEAN = -3\n",
    "LOG_STD_INIT_STD = 0.1\n",
    "LOG_STD_CLAMP = (-5, 2)\n",
    "BNN_EPOCHS = 500\n",
    "LEARNING_RATE = 0.01\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "AL_EPOCHS = 10\n",
    "N_SAMPLES = 5\n",
    "N_MC_SAMPLES = 100\n",
    "\n",
    "# Run active learning\n",
    "metrics_bnn, final_model_bnn = active_learning_loop_BNN(\n",
    "    scaled_x_train, scaled_y_train,\n",
    "    scaled_x_test, scaled_y_test,\n",
    "    scaled_x_remain, scaled_y_remain,\n",
    "    n_features=scaled_x_train.shape[1],\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    weight_init_std=WEIGHT_INIT_STD,\n",
    "    log_std_init_mean=LOG_STD_INIT_MEAN,\n",
    "    log_std_init_std=LOG_STD_INIT_STD,\n",
    "    log_std_clamp=LOG_STD_CLAMP,\n",
    "    bnn_epochs=BNN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    grad_clip_norm=GRAD_CLIP_NORM,\n",
    "    al_epochs=AL_EPOCHS,\n",
    "    n_samples=N_SAMPLES,\n",
    "    n_mc_samples=N_MC_SAMPLES\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Active Learning Summary ===\")\n",
    "print(f\"Initial Training Size: {len(scaled_x_train)}\")\n",
    "print(f\"Final Training Size: {metrics_bnn['training_size'][-1]}\")\n",
    "print(f\"Final Test Size: {metrics_bnn['test_size'][-1]}\")\n",
    "print(f\"Minimum RMSE: {min(metrics_bnn['rmse']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097b938-89cb-4c0c-8caa-c733dbb748c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "RMSE = []\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_bnn['training_size'], metrics_bnn['rmse'], \n",
    "         marker='o', markersize=6, \n",
    "         linestyle='-', color='red', \n",
    "         label='Active Learning', zorder=2)\n",
    "plt.plot(training_set, RMSE, \n",
    "         marker='s', markersize=6, \n",
    "         linestyle='-', color='blue', \n",
    "         label='Random Drop', zorder=4)\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Training Size', fontsize=16)\n",
    "plt.ylabel('RMSE on Testing Set', fontsize=16)\n",
    "plt.title('Active Learning vs Random Droping', fontsize=14)\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('al.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be08bd1-3635-4abf-a08f-1cf634e481f0",
   "metadata": {},
   "source": [
    "## MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c86de-d12c-4d19-839f-aeb4c932ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "HIDDEN_UNITS = [256, 512, 128]  # 3 hidden layers\n",
    "DROPOUT_RATE = 0.2\n",
    "MCD_EPOCHS = 1000\n",
    "AL_EPOCHS = 10\n",
    "\n",
    "# Run active learning\n",
    "metrics_mcd, mcd_model = active_learning_loop_MCD(\n",
    "    scaled_x_train, scaled_y_train,\n",
    "    scaled_x_test, scaled_y_test,\n",
    "    scaled_x_remain, scaled_y_remain,\n",
    "    n_features=scaled_x_train.shape[1],\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    mcd_epochs=MCD_EPOCHS,\n",
    "    learning_rate=0.001,\n",
    "    al_epochs=AL_EPOCHS,\n",
    "    n_samples=5,\n",
    "    T=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf8142-48f5-49cb-b4d9-f0fc03a7ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "RMSE = []\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_mcd['training_size'], metrics_mcd['rmse'], \n",
    "         marker='o', markersize=6, \n",
    "         linestyle='-', color='red', \n",
    "         label='Active Learning', zorder=2)\n",
    "plt.plot(training_set, RMSE, \n",
    "         marker='s', markersize=6, \n",
    "         linestyle='-', color='blue', \n",
    "         label='Random Drop', zorder=4)\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Training Size', fontsize=16)\n",
    "plt.ylabel('RMSE on Testing Set', fontsize=16)\n",
    "plt.title('Active Learning vs Random Droping', fontsize=14)\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('al.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
